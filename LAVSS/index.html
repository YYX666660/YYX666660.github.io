<html lang="en" class="LAVSS_tsinghua_vip">

<head>
    <!-- Required meta tags -->
    <title>Location-Guided Audio-Visual Spatial Audio Separation</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="keywords"
        content="video, sound, audio, deep learning, computer vision, machine learning">
    <meta name="description" content="n">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css">

    <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script>
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-81724582-4"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments) };
        gtag('js', new Date());
        gtag('config', 'UA-81724582-4');
    </script>

    <style>
        body {
            font-size: 16px
        }
        .navbar-fixed-top {
            min-height: 60px;
        }

        .navbar-nav>li>a {
            padding-top: 0px;
            padding-bottom: 0px;
            line-height: 60px;
            font-size: 22px;
            color:gray;
        }
        
        .navbar-nav>li>a:active {
            color:white;
        }
        
        .navbar-nav>li>a:hover {
            color:white;
            -webkit-tap-highlight-color: rgba(0,0,0,0);
            -webkit-tap-highlight-color:transparent;
            outline:none;
            background:none;
            text-decoration: none;
        }
        

    </style>
    <!-- Custom styles for this template -->
    <link href="jumbotron.css" rel="stylesheet">
</head>



<body data-gr-c-s-loaded="true">
    <nav class="navbar-fixed-top" style="background-color: rgb(44, 42, 42)">
        <a class="navbar-brand" href="#" style="font-size: 25px; color:white">LAVSS</a>

        <div class="collapse navbar-collapse" id="navbarsExampleDefault">
            <ul class="nav navbar-nav mr-auto" >
                <li><a href="#Video" style="font-size: 20px">Video</a></li>
                <li><a href="#Code" style="font-size: 20px">Code</a></li>
                <li><a href="#More" style="font-size: 20px">More</a></li>
            </ul>
        </div>
    </nav>


    <main role="main">
        <div class="container" style="padding-top: 80px; font-size: 20px">
            <div align="center">
                <h1 class="text-center" aligh="center">
                    Location-Guided Audio-Visual Spatial Audio Separation
                </h1><br>

         
	<br><br><br><br>
 	<center>
    	<img src="teaser_figure.png" width="900" height="300" alt="feature" />
    	</center>

                <br>
            </div>
        </div>

        <br>

        <div class="container">
            <h3 id="RFSleep" style="padding-top: 80px; margin-top: -80px;">Abstract:</h3>
            <hr>
            <div class="row">
                <div class="col-md-10 col-md-offset-1">
		<p style="text-align:justify">
                    The existing machine learning researches have achieved promising results in monaural audio-visual separation (MAVS) works. Most of the MAVS methods purely consider what but not where the sound source is. However, in VR/AR scenarios, this confounds listeners to distinguish similar audio sources in different directions. To alleviate this limitation, we generalize to spatial audio separation and propose LAVSS: a location-guided audio-visual spatial audio separator. Inspired from the correlation between spatial audio and visual location, we introduce the phase difference carried by the binaural audio as spatial cues. Simultaneously we utilize positional representations of sounding objects as an additional modality guidance. Furthermore, we leverage cross-modal attention to perform visual-positional collaboration with audio feature. In addition, a pre-trained monaural separator is adopted to transfer knowledge from rich mono sounds to boost spatial audio separation. This exploits the correlation between monaural and binaural channels. Experiments on FAIR-Play dataset demonstrate the superiority of proposed LAVSS over existing benchmarks of audio-visual separation.
                </div>
                <div style='margin-top: 280px'>
                 <div class="col-md-10 col-md-offset-1">
                </div>
            </div>
        </div>
        <br><br>


        <div class="container">
            <h3 id="Video" style="padding-top: 80px; margin-top: -80px;">Video:</h3>
            <hr>
            <div class="col-md-1"></div>
            <div class="col-md-10">
                <div class="embed-responsive embed-responsive-16by9">
                    <iframe src="https://player.bilibili.com/player.html?aid=615188298&bvid=BV14h4y1g7p8&cid=1176486836&page=1&high_quality=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
                </div>
            </div>
        </div><br><br>


        <div class="container">
            <h3 id="Code" style="padding-top: 80px; margin-top: -80px;">Code:</h3>
            <hr>
            <div class="row">
                <div class="col-md-9">
                        Code is avaliable.
                        </b><br></a>

                     <br>    
                    <a href="https://github.com/YYX666660/LAVSS">[Code]<br></a>
                </div>
            </div>
        </div><br><br>


        </div><br><br>
            
    </main>

</body>

</html>
